{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvised Previous Models and Better RMSE score \n",
    "- Shuffle= False in model.fit()  #Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.\n",
    "- increased epochs and batch size\n",
    "- [Scaling Test Data before Predictions](https://stackoverflow.com/questions/50565937/how-to-normalize-the-train-and-test-data-using-minmaxscaler-sklearn/50567308)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn .preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.10f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_excel('emotion.xls').drop('Date', axis=1 ).set_index(\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol =  data.iloc[ : , 0:13].drop(['% change_nifty', '% change_sensex'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3168 entries, 2007-04-19 to 2020-03-16\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Anger                   3168 non-null   float32\n",
      " 1   Anticipation            3168 non-null   float32\n",
      " 2   Disgust                 3168 non-null   float32\n",
      " 3   Fear                    3168 non-null   float32\n",
      " 4   Joy                     3168 non-null   float32\n",
      " 5   Sadness                 3168 non-null   float32\n",
      " 6   Surprise                3168 non-null   float32\n",
      " 7   Trust                   3168 non-null   float32\n",
      " 8   Close_nifty             3168 non-null   float32\n",
      " 9   Close_sensex            3168 non-null   float32\n",
      " 10  Conditional Volatility  3168 non-null   float32\n",
      "dtypes: float32(11)\n",
      "memory usage: 160.9 KB\n"
     ]
    }
   ],
   "source": [
    "data_vol.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Close_nifty</th>\n",
       "      <th>Close_sensex</th>\n",
       "      <th>Conditional Volatility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-04-19</th>\n",
       "      <td>0.5000000000</td>\n",
       "      <td>2.2100000381</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.5160000324</td>\n",
       "      <td>0.8130000234</td>\n",
       "      <td>0.4309999943</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0470000505</td>\n",
       "      <td>3,997.6499023438</td>\n",
       "      <td>13,619.7001953125</td>\n",
       "      <td>1.9240180254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-23</th>\n",
       "      <td>0.4850000143</td>\n",
       "      <td>1.3819999695</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.5780000091</td>\n",
       "      <td>1.0479999781</td>\n",
       "      <td>2.6700000763</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.1720000505</td>\n",
       "      <td>4,085.1000976562</td>\n",
       "      <td>13,928.3300781250</td>\n",
       "      <td>1.9290779829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-30</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>2.7019999027</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.2969999909</td>\n",
       "      <td>1.6189999580</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.6410000324</td>\n",
       "      <td>4,087.8999023438</td>\n",
       "      <td>13,872.3701171875</td>\n",
       "      <td>2.1650118828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-07</th>\n",
       "      <td>0.5310000181</td>\n",
       "      <td>3.1619999409</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.2969999909</td>\n",
       "      <td>2.6429998875</td>\n",
       "      <td>0.2029999942</td>\n",
       "      <td>0.3670000136</td>\n",
       "      <td>3.5710000992</td>\n",
       "      <td>4,111.1499023438</td>\n",
       "      <td>13,879.2500000000</td>\n",
       "      <td>2.0097639561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-08</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0069999695</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.8930000067</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0470000505</td>\n",
       "      <td>4,077.0000000000</td>\n",
       "      <td>13,765.4599609375</td>\n",
       "      <td>1.8497519493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Anger  Anticipation      Disgust         Fear          Joy  \\\n",
       "day                                                                            \n",
       "2007-04-19 0.5000000000  2.2100000381 0.0000000000 1.5160000324 0.8130000234   \n",
       "2007-04-23 0.4850000143  1.3819999695 0.0000000000 0.5780000091 1.0479999781   \n",
       "2007-04-30 0.0000000000  2.7019999027 0.0000000000 0.2969999909 1.6189999580   \n",
       "2007-05-07 0.5310000181  3.1619999409 0.0000000000 0.2969999909 2.6429998875   \n",
       "2007-05-08 0.0000000000  1.0069999695 0.0000000000 0.0000000000 0.8930000067   \n",
       "\n",
       "                Sadness     Surprise        Trust      Close_nifty  \\\n",
       "day                                                                  \n",
       "2007-04-19 0.4309999943 0.0000000000 1.0470000505 3,997.6499023438   \n",
       "2007-04-23 2.6700000763 0.0000000000 1.1720000505 4,085.1000976562   \n",
       "2007-04-30 0.0000000000 0.0000000000 1.6410000324 4,087.8999023438   \n",
       "2007-05-07 0.2029999942 0.3670000136 3.5710000992 4,111.1499023438   \n",
       "2007-05-08 0.0000000000 0.0000000000 1.0470000505 4,077.0000000000   \n",
       "\n",
       "                Close_sensex  Conditional Volatility  \n",
       "day                                                   \n",
       "2007-04-19 13,619.7001953125            1.9240180254  \n",
       "2007-04-23 13,928.3300781250            1.9290779829  \n",
       "2007-04-30 13,872.3701171875            2.1650118828  \n",
       "2007-05-07 13,879.2500000000            2.0097639561  \n",
       "2007-05-08 13,765.4599609375            1.8497519493  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_dataset(data):\n",
    "# \t# split into  months\n",
    "#     train, test = data[0:3138], data[3138:3168]\n",
    "# \t# restructure into windows of monthly data\n",
    "#     train = array(split(train, len(train)/30))\n",
    "#     test = array(split(test, len(test)/30))\n",
    "#     return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yscaler = MinMaxScaler()\n",
    "df = pd.DataFrame(data_vol.iloc[18:3138, 10])\n",
    "df = pd.DataFrame(yscaler.fit_transform(df), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train, test = data_vol[18:3138], data_vol[3138:3168]\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(scaler.fit_transform(test), columns=test.columns, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Close_nifty</th>\n",
       "      <th>Close_sensex</th>\n",
       "      <th>Conditional Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0667189583</td>\n",
       "      <td>0.1310477257</td>\n",
       "      <td>0.0656884834</td>\n",
       "      <td>0.0793018043</td>\n",
       "      <td>0.1180894151</td>\n",
       "      <td>0.1094094813</td>\n",
       "      <td>0.0957049057</td>\n",
       "      <td>0.0974604413</td>\n",
       "      <td>0.4660172760</td>\n",
       "      <td>0.4551837146</td>\n",
       "      <td>0.0627599508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.1343169659</td>\n",
       "      <td>0.1967794001</td>\n",
       "      <td>0.1465145200</td>\n",
       "      <td>0.1510540098</td>\n",
       "      <td>0.1811063737</td>\n",
       "      <td>0.1672122926</td>\n",
       "      <td>0.1806580275</td>\n",
       "      <td>0.1728436798</td>\n",
       "      <td>0.2494771630</td>\n",
       "      <td>0.2389431000</td>\n",
       "      <td>0.1095687598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0102568883</td>\n",
       "      <td>0.0386259221</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0146211691</td>\n",
       "      <td>0.0332342461</td>\n",
       "      <td>0.0263262000</td>\n",
       "      <td>0.0130911404</td>\n",
       "      <td>0.0220476161</td>\n",
       "      <td>0.2735500485</td>\n",
       "      <td>0.2722832114</td>\n",
       "      <td>0.0140771582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0205137767</td>\n",
       "      <td>0.0591193344</td>\n",
       "      <td>0.0147001697</td>\n",
       "      <td>0.0273765000</td>\n",
       "      <td>0.0526462533</td>\n",
       "      <td>0.0504153557</td>\n",
       "      <td>0.0318445377</td>\n",
       "      <td>0.0353673287</td>\n",
       "      <td>0.3654592931</td>\n",
       "      <td>0.3632382154</td>\n",
       "      <td>0.0270586554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0390222305</td>\n",
       "      <td>0.1055044997</td>\n",
       "      <td>0.0361902015</td>\n",
       "      <td>0.0472020544</td>\n",
       "      <td>0.0934748799</td>\n",
       "      <td>0.0913505517</td>\n",
       "      <td>0.0642666277</td>\n",
       "      <td>0.0630328879</td>\n",
       "      <td>0.6322714686</td>\n",
       "      <td>0.6030966938</td>\n",
       "      <td>0.0569391027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.9999999404</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anger     Anticipation          Disgust             Fear  \\\n",
       "count 3,120.0000000000 3,120.0000000000 3,120.0000000000 3,120.0000000000   \n",
       "mean      0.0667189583     0.1310477257     0.0656884834     0.0793018043   \n",
       "std       0.1343169659     0.1967794001     0.1465145200     0.1510540098   \n",
       "min       0.0000000000     0.0000000000     0.0000000000     0.0000000000   \n",
       "25%       0.0102568883     0.0386259221     0.0000000000     0.0146211691   \n",
       "50%       0.0205137767     0.0591193344     0.0147001697     0.0273765000   \n",
       "75%       0.0390222305     0.1055044997     0.0361902015     0.0472020544   \n",
       "max       0.9999999404     1.0000000000     1.0000000000     1.0000000000   \n",
       "\n",
       "                   Joy          Sadness         Surprise            Trust  \\\n",
       "count 3,120.0000000000 3,120.0000000000 3,120.0000000000 3,120.0000000000   \n",
       "mean      0.1180894151     0.1094094813     0.0957049057     0.0974604413   \n",
       "std       0.1811063737     0.1672122926     0.1806580275     0.1728436798   \n",
       "min       0.0000000000     0.0000000000     0.0000000000     0.0000000000   \n",
       "25%       0.0332342461     0.0263262000     0.0130911404     0.0220476161   \n",
       "50%       0.0526462533     0.0504153557     0.0318445377     0.0353673287   \n",
       "75%       0.0934748799     0.0913505517     0.0642666277     0.0630328879   \n",
       "max       1.0000000000     1.0000000000     1.0000000000     1.0000000000   \n",
       "\n",
       "           Close_nifty     Close_sensex  Conditional Volatility  \n",
       "count 3,120.0000000000 3,120.0000000000        3,120.0000000000  \n",
       "mean      0.4660172760     0.4551837146            0.0627599508  \n",
       "std       0.2494771630     0.2389431000            0.1095687598  \n",
       "min       0.0000000000     0.0000000000            0.0000000000  \n",
       "25%       0.2735500485     0.2722832114            0.0140771582  \n",
       "50%       0.3654592931     0.3632382154            0.0270586554  \n",
       "75%       0.6322714686     0.6030966938            0.0569391027  \n",
       "max       1.0000000000     1.0000000000            1.0000000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Close_nifty</th>\n",
       "      <th>Close_sensex</th>\n",
       "      <th>Conditional Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.5515988469</td>\n",
       "      <td>0.5071460009</td>\n",
       "      <td>0.4846850038</td>\n",
       "      <td>0.5973227024</td>\n",
       "      <td>0.5359319448</td>\n",
       "      <td>0.4792217612</td>\n",
       "      <td>0.4619318247</td>\n",
       "      <td>0.3523563147</td>\n",
       "      <td>0.7586560845</td>\n",
       "      <td>0.7619783282</td>\n",
       "      <td>0.1437718421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.2692614794</td>\n",
       "      <td>0.2503173649</td>\n",
       "      <td>0.2438482940</td>\n",
       "      <td>0.2421970218</td>\n",
       "      <td>0.2378204912</td>\n",
       "      <td>0.2652696371</td>\n",
       "      <td>0.2123727500</td>\n",
       "      <td>0.2050618678</td>\n",
       "      <td>0.2697877586</td>\n",
       "      <td>0.2673042417</td>\n",
       "      <td>0.2423311025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.3426034003</td>\n",
       "      <td>0.3419924080</td>\n",
       "      <td>0.2841547430</td>\n",
       "      <td>0.4213398844</td>\n",
       "      <td>0.3571308702</td>\n",
       "      <td>0.2642595172</td>\n",
       "      <td>0.3399322331</td>\n",
       "      <td>0.2331271619</td>\n",
       "      <td>0.6713703275</td>\n",
       "      <td>0.6815431118</td>\n",
       "      <td>0.0386144165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.5611848235</td>\n",
       "      <td>0.5451586843</td>\n",
       "      <td>0.5261563659</td>\n",
       "      <td>0.6132943630</td>\n",
       "      <td>0.5389156938</td>\n",
       "      <td>0.4636054635</td>\n",
       "      <td>0.4811083972</td>\n",
       "      <td>0.3379986882</td>\n",
       "      <td>0.8507556915</td>\n",
       "      <td>0.8544769287</td>\n",
       "      <td>0.0506458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.7835264802</td>\n",
       "      <td>0.6754058301</td>\n",
       "      <td>0.6659611762</td>\n",
       "      <td>0.7706734836</td>\n",
       "      <td>0.7058766186</td>\n",
       "      <td>0.6165616512</td>\n",
       "      <td>0.5351662338</td>\n",
       "      <td>0.4581854045</td>\n",
       "      <td>0.9620062113</td>\n",
       "      <td>0.9583874941</td>\n",
       "      <td>0.1325604618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>0.9999999404</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Anger  Anticipation       Disgust          Fear           Joy  \\\n",
       "count 30.0000000000 30.0000000000 30.0000000000 30.0000000000 30.0000000000   \n",
       "mean   0.5515988469  0.5071460009  0.4846850038  0.5973227024  0.5359319448   \n",
       "std    0.2692614794  0.2503173649  0.2438482940  0.2421970218  0.2378204912   \n",
       "min    0.0000000000  0.0000000000  0.0000000000  0.0000000000  0.0000000000   \n",
       "25%    0.3426034003  0.3419924080  0.2841547430  0.4213398844  0.3571308702   \n",
       "50%    0.5611848235  0.5451586843  0.5261563659  0.6132943630  0.5389156938   \n",
       "75%    0.7835264802  0.6754058301  0.6659611762  0.7706734836  0.7058766186   \n",
       "max    1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000   \n",
       "\n",
       "            Sadness      Surprise         Trust   Close_nifty  Close_sensex  \\\n",
       "count 30.0000000000 30.0000000000 30.0000000000 30.0000000000 30.0000000000   \n",
       "mean   0.4792217612  0.4619318247  0.3523563147  0.7586560845  0.7619783282   \n",
       "std    0.2652696371  0.2123727500  0.2050618678  0.2697877586  0.2673042417   \n",
       "min    0.0000000000  0.0000000000  0.0000000000  0.0000000000  0.0000000000   \n",
       "25%    0.2642595172  0.3399322331  0.2331271619  0.6713703275  0.6815431118   \n",
       "50%    0.4636054635  0.4811083972  0.3379986882  0.8507556915  0.8544769287   \n",
       "75%    0.6165616512  0.5351662338  0.4581854045  0.9620062113  0.9583874941   \n",
       "max    0.9999999404  1.0000000000  1.0000000000  1.0000000000  1.0000000000   \n",
       "\n",
       "       Conditional Volatility  \n",
       "count           30.0000000000  \n",
       "mean             0.1437718421  \n",
       "std              0.2423311025  \n",
       "min              0.0000000000  \n",
       "25%              0.0386144165  \n",
       "50%              0.0506458730  \n",
       "75%              0.1325604618  \n",
       "max              1.0000000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = array(split(train.values, len(train)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = array(split(test.values, len(test)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# for col in data_vol.columns :\n",
    "# # for i in range (0,train.shape[2]) :\n",
    "#     data_vol[[col]] = scaler.fit_transform(data_vol[[col]])\n",
    "# #     train[:,:, i] = scaler.fit_transform(train[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(train, n_input, n_out=30):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\tfor _ in range(len(data)):\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 10])\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, n_input):\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\tverbose, epochs, batch_size = 1, 300, 100\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\tprint(train_x.shape)\n",
    "\tprint(train_y.shape)\n",
    "    \n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(200, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\tmodel.summary()\n",
    "    \n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, shuffle= False )\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3061, 30, 11)\n",
      "(3061, 30, 1)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               169600    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 30, 200)           320800    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 30, 100)           20100     \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 30, 1)             101       \n",
      "=================================================================\n",
      "Total params: 510,601\n",
      "Trainable params: 510,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0147\n",
      "Epoch 2/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0138\n",
      "Epoch 3/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0087\n",
      "Epoch 4/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0119\n",
      "Epoch 5/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0082\n",
      "Epoch 6/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0071\n",
      "Epoch 7/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0076\n",
      "Epoch 8/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0067\n",
      "Epoch 9/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0059\n",
      "Epoch 10/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0082\n",
      "Epoch 11/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0054\n",
      "Epoch 12/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0053\n",
      "Epoch 13/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0053\n",
      "Epoch 14/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0052\n",
      "Epoch 15/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0051\n",
      "Epoch 16/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0053\n",
      "Epoch 17/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0054\n",
      "Epoch 18/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0070\n",
      "Epoch 19/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0055\n",
      "Epoch 20/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0058\n",
      "Epoch 21/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0052\n",
      "Epoch 22/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0057\n",
      "Epoch 23/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0051\n",
      "Epoch 24/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0054\n",
      "Epoch 25/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0050\n",
      "Epoch 26/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0054\n",
      "Epoch 27/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0054\n",
      "Epoch 28/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0068\n",
      "Epoch 29/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0052\n",
      "Epoch 30/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0055\n",
      "Epoch 31/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0050\n",
      "Epoch 32/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0051\n",
      "Epoch 33/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0050\n",
      "Epoch 34/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0060\n",
      "Epoch 35/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0050\n",
      "Epoch 36/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0051\n",
      "Epoch 37/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0048\n",
      "Epoch 38/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0053\n",
      "Epoch 39/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0048\n",
      "Epoch 40/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0051\n",
      "Epoch 41/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0047\n",
      "Epoch 42/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0052\n",
      "Epoch 43/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0050\n",
      "Epoch 44/300\n",
      "3061/3061 [==============================] - 20s 7ms/step - loss: 0.0058\n",
      "Epoch 45/300\n",
      "3061/3061 [==============================] - 21s 7ms/step - loss: 0.0051\n",
      "Epoch 46/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0061\n",
      "Epoch 47/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0048\n",
      "Epoch 48/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0050\n",
      "Epoch 49/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0046\n",
      "Epoch 50/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0053\n",
      "Epoch 51/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0047\n",
      "Epoch 52/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0052\n",
      "Epoch 53/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0048\n",
      "Epoch 54/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0056\n",
      "Epoch 55/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0048\n",
      "Epoch 56/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0054\n",
      "Epoch 57/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0046\n",
      "Epoch 58/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0046\n",
      "Epoch 59/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0043\n",
      "Epoch 60/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0049\n",
      "Epoch 61/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0049\n",
      "Epoch 62/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0063\n",
      "Epoch 63/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0046\n",
      "Epoch 64/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0045\n",
      "Epoch 65/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0043\n",
      "Epoch 66/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0044\n",
      "Epoch 67/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0042\n",
      "Epoch 68/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0049\n",
      "Epoch 69/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0046\n",
      "Epoch 70/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0054\n",
      "Epoch 71/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0043\n",
      "Epoch 72/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0046\n",
      "Epoch 73/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0042\n",
      "Epoch 74/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0048\n",
      "Epoch 75/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0048\n",
      "Epoch 76/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0061\n",
      "Epoch 77/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 1.0363\n",
      "Epoch 78/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0143\n",
      "Epoch 79/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0117\n",
      "Epoch 80/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0078\n",
      "Epoch 81/300\n",
      "3061/3061 [==============================] - 14s 5ms/step - loss: 0.0098\n",
      "Epoch 82/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0069\n",
      "Epoch 83/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0054\n",
      "Epoch 84/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0065\n",
      "Epoch 85/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0050\n",
      "Epoch 86/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0049\n",
      "Epoch 87/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0048\n",
      "Epoch 88/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0048\n",
      "Epoch 89/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0047\n",
      "Epoch 90/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0047\n",
      "Epoch 91/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0046\n",
      "Epoch 92/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0047\n",
      "Epoch 93/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0046\n",
      "Epoch 94/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0047\n",
      "Epoch 95/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0045\n",
      "Epoch 96/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0047\n",
      "Epoch 97/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0045\n",
      "Epoch 98/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0046\n",
      "Epoch 99/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0044\n",
      "Epoch 100/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0046\n",
      "Epoch 101/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0044\n",
      "Epoch 102/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0047\n",
      "Epoch 103/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0044\n",
      "Epoch 104/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0046\n",
      "Epoch 105/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0043\n",
      "Epoch 106/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0046\n",
      "Epoch 107/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0044\n",
      "Epoch 108/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0046\n",
      "Epoch 109/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0043\n",
      "Epoch 110/300\n",
      "3061/3061 [==============================] - 20s 7ms/step - loss: 0.0045\n",
      "Epoch 111/300\n",
      "3061/3061 [==============================] - 21s 7ms/step - loss: 0.0043\n",
      "Epoch 112/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0046\n",
      "Epoch 113/300\n",
      "3061/3061 [==============================] - 21s 7ms/step - loss: 0.0044\n",
      "Epoch 114/300\n",
      "3061/3061 [==============================] - 20s 7ms/step - loss: 0.0044\n",
      "Epoch 115/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0042\n",
      "Epoch 116/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0045\n",
      "Epoch 117/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0041\n",
      "Epoch 118/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0044\n",
      "Epoch 119/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0043\n",
      "Epoch 120/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0050\n",
      "Epoch 121/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0043\n",
      "Epoch 122/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0043\n",
      "Epoch 123/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0041\n",
      "Epoch 124/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0040\n",
      "Epoch 125/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0040\n",
      "Epoch 126/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0048\n",
      "Epoch 127/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0045\n",
      "Epoch 128/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0054\n",
      "Epoch 129/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0043\n",
      "Epoch 130/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0039\n",
      "Epoch 131/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0038\n",
      "Epoch 132/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0039\n",
      "Epoch 133/300\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0046\n",
      "Epoch 134/300\n",
      "3061/3061 [==============================] - 21s 7ms/step - loss: 0.0060\n",
      "Epoch 135/300\n",
      "3061/3061 [==============================] - 21s 7ms/step - loss: 0.0053\n",
      "Epoch 136/300\n",
      "3061/3061 [==============================] - 21s 7ms/step - loss: 0.0048\n",
      "Epoch 137/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0045\n",
      "Epoch 138/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0045\n",
      "Epoch 139/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0043\n",
      "Epoch 140/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0043\n",
      "Epoch 141/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0042\n",
      "Epoch 142/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0041\n",
      "Epoch 143/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0040\n",
      "Epoch 144/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0040\n",
      "Epoch 145/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0041\n",
      "Epoch 146/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0042\n",
      "Epoch 147/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0042\n",
      "Epoch 148/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0045\n",
      "Epoch 149/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0040\n",
      "Epoch 150/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0038\n",
      "Epoch 151/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0036\n",
      "Epoch 152/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0036\n",
      "Epoch 153/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0041\n",
      "Epoch 154/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0047\n",
      "Epoch 155/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0040\n",
      "Epoch 156/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0036\n",
      "Epoch 157/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0035\n",
      "Epoch 158/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0033\n",
      "Epoch 159/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0034\n",
      "Epoch 160/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0038\n",
      "Epoch 161/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0042\n",
      "Epoch 162/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0067\n",
      "Epoch 163/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0045\n",
      "Epoch 164/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0040\n",
      "Epoch 165/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0039\n",
      "Epoch 166/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0035\n",
      "Epoch 167/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0035\n",
      "Epoch 168/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0032\n",
      "Epoch 169/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0033\n",
      "Epoch 170/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0046\n",
      "Epoch 171/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0056\n",
      "Epoch 172/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0048\n",
      "Epoch 173/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0041\n",
      "Epoch 174/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0037\n",
      "Epoch 175/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0036\n",
      "Epoch 176/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0035\n",
      "Epoch 177/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0037\n",
      "Epoch 178/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0045\n",
      "Epoch 179/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0054\n",
      "Epoch 180/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0037\n",
      "Epoch 181/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0034\n",
      "Epoch 182/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0032\n",
      "Epoch 183/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0031\n",
      "Epoch 184/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0029\n",
      "Epoch 185/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0028\n",
      "Epoch 186/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0027\n",
      "Epoch 187/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0042\n",
      "Epoch 188/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0041\n",
      "Epoch 189/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0049\n",
      "Epoch 190/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0038\n",
      "Epoch 191/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0034\n",
      "Epoch 192/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0031\n",
      "Epoch 193/300\n",
      "3061/3061 [==============================] - 14s 5ms/step - loss: 0.0029\n",
      "Epoch 194/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0028\n",
      "Epoch 195/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0034\n",
      "Epoch 196/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0042\n",
      "Epoch 197/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0054\n",
      "Epoch 198/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0050\n",
      "Epoch 199/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0038\n",
      "Epoch 200/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0031\n",
      "Epoch 201/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0032\n",
      "Epoch 202/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0034\n",
      "Epoch 203/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0038\n",
      "Epoch 204/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0032\n",
      "Epoch 205/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0033\n",
      "Epoch 206/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0030\n",
      "Epoch 207/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0026\n",
      "Epoch 208/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0026\n",
      "Epoch 209/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0027\n",
      "Epoch 210/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0038\n",
      "Epoch 211/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0044\n",
      "Epoch 212/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0032\n",
      "Epoch 213/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0030\n",
      "Epoch 214/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0026\n",
      "Epoch 215/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0026\n",
      "Epoch 216/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0026\n",
      "Epoch 217/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0032\n",
      "Epoch 218/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0030\n",
      "Epoch 219/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0040\n",
      "Epoch 220/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0046\n",
      "Epoch 221/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0045\n",
      "Epoch 222/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0032\n",
      "Epoch 223/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0029\n",
      "Epoch 224/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0025\n",
      "Epoch 225/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0024\n",
      "Epoch 226/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0022\n",
      "Epoch 227/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0028\n",
      "Epoch 238/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0031\n",
      "Epoch 239/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0028\n",
      "Epoch 240/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0032\n",
      "Epoch 241/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0026\n",
      "Epoch 242/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0026\n",
      "Epoch 243/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0023\n",
      "Epoch 244/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0025\n",
      "Epoch 245/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0025\n",
      "Epoch 246/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0028\n",
      "Epoch 247/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0023\n",
      "Epoch 248/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0025\n",
      "Epoch 249/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0037\n",
      "Epoch 250/300\n",
      "3061/3061 [==============================] - 20s 7ms/step - loss: 0.0036\n",
      "Epoch 251/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0022\n",
      "Epoch 252/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0021\n",
      "Epoch 253/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0024\n",
      "Epoch 254/300\n",
      "3061/3061 [==============================] - 14s 5ms/step - loss: 0.0023\n",
      "Epoch 255/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0021\n",
      "Epoch 256/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0021\n",
      "Epoch 257/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0020\n",
      "Epoch 258/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0027\n",
      "Epoch 259/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0026\n",
      "Epoch 260/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0038\n",
      "Epoch 261/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0031\n",
      "Epoch 262/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0027\n",
      "Epoch 263/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0030\n",
      "Epoch 264/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0025\n",
      "Epoch 265/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0024\n",
      "Epoch 266/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0024\n",
      "Epoch 267/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0026\n",
      "Epoch 268/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0021\n",
      "Epoch 269/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0023\n",
      "Epoch 270/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0019\n",
      "Epoch 271/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0023\n",
      "Epoch 272/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0020\n",
      "Epoch 273/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0027\n",
      "Epoch 274/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0021\n",
      "Epoch 275/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0024\n",
      "Epoch 276/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0020\n",
      "Epoch 277/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0026\n",
      "Epoch 278/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0023\n",
      "Epoch 279/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0035\n",
      "Epoch 280/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0023\n",
      "Epoch 281/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0026\n",
      "Epoch 282/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0021\n",
      "Epoch 283/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0020\n",
      "Epoch 284/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0018\n",
      "Epoch 285/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0019\n",
      "Epoch 286/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0018\n",
      "Epoch 287/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0033\n",
      "Epoch 288/300\n",
      "3061/3061 [==============================] - 15s 5ms/step - loss: 0.0031\n",
      "Epoch 289/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0021\n",
      "Epoch 290/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0017\n",
      "Epoch 291/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0019\n",
      "Epoch 292/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0020\n",
      "Epoch 293/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0023\n",
      "Epoch 294/300\n",
      "3061/3061 [==============================] - 19s 6ms/step - loss: 0.0019\n",
      "Epoch 295/300\n",
      "3061/3061 [==============================] - 17s 5ms/step - loss: 0.0041\n",
      "Epoch 296/300\n",
      "3061/3061 [==============================] - 16s 5ms/step - loss: 0.0030\n",
      "Epoch 297/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0023\n",
      "Epoch 298/300\n",
      "3061/3061 [==============================] - 20s 6ms/step - loss: 0.0024\n",
      "Epoch 299/300\n",
      "3061/3061 [==============================] - 18s 6ms/step - loss: 0.0021\n",
      "Epoch 300/300\n",
      "3061/3061 [==============================] - 17s 6ms/step - loss: 0.0033\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "n_input = 30 \n",
    "model = build_model(train, n_input) \n",
    "model.save( 'model5-copy2' + '.h5')\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model  \n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model5-copy2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecasts_new(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "        \n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_new(train, test, n_input):\n",
    "    \n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "    \n",
    "\t# walk-forward validation \n",
    "\tpredictions = list()\n",
    "    \n",
    "\tfor i in range(len(test)):\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "# \t\tprint(type(yhat_sequence))\n",
    "# \t\tprint(yhat_sequence.shape) \n",
    "# \t\ty1 = scaler.inverse_transform(yhat_sequence)\n",
    "# \t\tprint(type(y1))\n",
    "# \t\tprint(y1.shape)    \n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\thistory.append(test[i, :])\n",
    "        \n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "# \tprint(predictions.shape)  \n",
    "# \tpredictions = scaler.inverse_transform(predictions[0,:,:])\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 717ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = evaluate_model_new(train, test, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48979443 0.5681981  0.4947866  0.4829613  0.44494575 0.4523285\n",
      "  0.44238916 0.27872235 0.97078705 0.96971655 0.03399805]]\n"
     ]
    }
   ],
   "source": [
    "print(test[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.01868879]\n",
      "  [0.01977039]\n",
      "  [0.01935527]\n",
      "  [0.01970544]\n",
      "  [0.02059191]\n",
      "  [0.02178053]\n",
      "  [0.02307583]\n",
      "  [0.02329719]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores = evaluate_forecasts_new(test[:, :, 8], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: [0.783] 0.8, 0.8, 0.9, 0.9, 1.0, 0.9, 0.9, 0.9, 1.0, 1.0, 0.9, 0.9, 0.9, 1.0, 0.9, 0.9, 0.8, 0.8, 0.8, 0.6, 0.6, 0.7, 0.7, 0.7, 0.6, 0.4, 0.4, 0.1, 0.2, 0.0\n"
     ]
    }
   ],
   "source": [
    "summarize_scores('test', score, scores)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
