{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention LSTM \n",
    "    - 30 day Rolling Window Forecast using Attention LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "# from attention_decoder import AttentionDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn .preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.10f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_excel('emotion.xls').drop('Date', axis=1 ).set_index(\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol =  data.iloc[ : , 0:13].drop(['Close_nifty', 'Close_sensex','% change_nifty', '% change_sensex'], axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PREP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train, test = data_vol[18:3138], data_vol[3138:3168]\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yscaler = MinMaxScaler()\n",
    "df = pd.DataFrame(data_vol.iloc[18:3138, 8])\n",
    "df = pd.DataFrame(yscaler.fit_transform(df), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = array(split(train.values, len(train)/30))\n",
    "test = array(split(test.values, len(test)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(train, n_input, n_out=30):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\tfor _ in range(len(data)):\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 8])\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Layer  \n",
    "from keras import initializers, regularizers, constraints  \n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        # todo: check that this is correct\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True,\n",
    "                 return_attention=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Note: The layer has been tested with Keras 1.x\n",
    "        Example:\n",
    "        \n",
    "            # 1\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "            # next add a Dense layer (for classification/regression) or whatever...\n",
    "            # 2 - Get the attention scores\n",
    "            hidden = LSTM(64, return_sequences=True)(words)\n",
    "            sentence, word_scores = Attention(return_attention=True)(hidden)\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape = (input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape = (input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        weighted_input = x * K.expand_dims(a)\n",
    "\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return [result, a]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], input_shape[-1]),\n",
    "                    (input_shape[0], input_shape[1])]\n",
    "        else:\n",
    "            return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, n_input):\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\tverbose, epochs, batch_size = 1, 55, 20\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\tprint(train_x.shape)\n",
    "\tprint(train_y.shape)\n",
    "    \n",
    "\tunits= 200\n",
    "\tmodel = Sequential() \n",
    "\tmodel.add(LSTM(units, activation='relu', input_shape=(n_timesteps, n_features), return_sequences = True))#output:(none, timesteps,units)\n",
    "\tmodel.add(Attention())\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(Dense(1))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\tmodel.summary()\n",
    "\n",
    "    \n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3061, 30, 9)\n",
      "(3061, 30, 1)\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 30, 200)           168000    \n",
      "_________________________________________________________________\n",
      "attention_9 (Attention)      (None, 200)               230       \n",
      "_________________________________________________________________\n",
      "repeat_vector_5 (RepeatVecto (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 30, 1)             201       \n",
      "=================================================================\n",
      "Total params: 168,431\n",
      "Trainable params: 168,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0074\n",
      "Epoch 2/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0060\n",
      "Epoch 3/55\n",
      "3061/3061 [==============================] - 26s 8ms/step - loss: 0.0057\n",
      "Epoch 4/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0052\n",
      "Epoch 5/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0054\n",
      "Epoch 6/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0052\n",
      "Epoch 7/55\n",
      "3061/3061 [==============================] - 25s 8ms/step - loss: 0.0049\n",
      "Epoch 8/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0047\n",
      "Epoch 9/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0047\n",
      "Epoch 10/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0045\n",
      "Epoch 11/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0045\n",
      "Epoch 12/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0044\n",
      "Epoch 13/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0041\n",
      "Epoch 14/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0042\n",
      "Epoch 15/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0037\n",
      "Epoch 16/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0038\n",
      "Epoch 17/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0037\n",
      "Epoch 18/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0036\n",
      "Epoch 19/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0034\n",
      "Epoch 20/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0041\n",
      "Epoch 21/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0029\n",
      "Epoch 22/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0029\n",
      "Epoch 23/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0032\n",
      "Epoch 24/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0026\n",
      "Epoch 25/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0029\n",
      "Epoch 26/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0028\n",
      "Epoch 27/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0028\n",
      "Epoch 28/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0027\n",
      "Epoch 29/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0028\n",
      "Epoch 30/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0043\n",
      "Epoch 31/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0029\n",
      "Epoch 32/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0025\n",
      "Epoch 33/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0025\n",
      "Epoch 34/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0024\n",
      "Epoch 35/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0023\n",
      "Epoch 36/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0026\n",
      "Epoch 37/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0023\n",
      "Epoch 38/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0022\n",
      "Epoch 39/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0022\n",
      "Epoch 40/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0022\n",
      "Epoch 41/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0022\n",
      "Epoch 42/55\n",
      "3061/3061 [==============================] - 25s 8ms/step - loss: 0.0021\n",
      "Epoch 43/55\n",
      "3061/3061 [==============================] - 25s 8ms/step - loss: 0.0021\n",
      "Epoch 44/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0036\n",
      "Epoch 45/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0024\n",
      "Epoch 46/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0022\n",
      "Epoch 47/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0021\n",
      "Epoch 48/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0021\n",
      "Epoch 49/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0021\n",
      "Epoch 50/55\n",
      "3061/3061 [==============================] - 23s 7ms/step - loss: 0.0021\n",
      "Epoch 51/55\n",
      "3061/3061 [==============================] - 23s 8ms/step - loss: 0.0021\n",
      "Epoch 52/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0021\n",
      "Epoch 53/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0021\n",
      "Epoch 54/55\n",
      "3061/3061 [==============================] - 24s 8ms/step - loss: 0.0026\n",
      "Epoch 55/55\n",
      "3061/3061 [==============================] - 22s 7ms/step - loss: 0.0021\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "n_input = 30 \n",
    "model = build_model(train, n_input) \n",
    "model.save( 'model8' + '.h5')\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model  \n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecasts_new(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "        \n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_new(train, test, n_input):\n",
    "    \n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "    \n",
    "\t# walk-forward validation \n",
    "\tpredictions = list()\n",
    "    \n",
    "\tfor i in range(len(test)):\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)   \n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\thistory.append(test[i, :])\n",
    "        \n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = evaluate_model_new(train, test, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape\n",
    "predictions = yscaler.inverse_transform(preds[0,:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores = evaluate_forecasts_new(test[:, :, 8], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: [2.607] 0.1, 0.4, 0.4, 0.7, 0.6, 0.6, 0.5, 0.4, 0.4, 0.3, 0.3, 0.2, 0.2, 0.2, 0.2, 0.1, 0.4, 0.4, 0.4, 0.3, 1.4, 1.3, 1.2, 1.1, 1.0, 1.3, 3.5, 3.2, 9.0, 9.4\n"
     ]
    }
   ],
   "source": [
    "summarize_scores('test', score, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **References** :\n",
    "\n",
    "To implement Attention Mechanism I used the [source code](https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d) of Christos Baziotis. A few changes very made to debug errors [using this gist comment](https://gist.github.com/cbaziotis/6428df359af27d58078ca5ed9792bd6d#gistcomment-3094589 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
