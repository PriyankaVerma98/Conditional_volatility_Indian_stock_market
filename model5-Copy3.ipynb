{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvised Previous Models and Better RMSE score \n",
    "- Shuffle= False in model.fit()  #Keras shuffles the training dataset before each training epoch. To ensure the training data patterns remain sequential, we can disable this shuffling.\n",
    "- increased epochs and batch size\n",
    "- [Scaling Test Data before Predictions](https://stackoverflow.com/questions/50565937/how-to-normalize-the-train-and-test-data-using-minmaxscaler-sklearn/50567308)\n",
    "- Increased the no. of Neurons : Generally, more neurons would be able to learn more structure from the problem at the cost of longer training time. \n",
    "- Dropouts added : More learning capacity also creates the problem of potentially overfitting the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn .preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import split\n",
    "from numpy import array\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:,.10f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_excel('emotion.xls').drop('Date', axis=1 ).set_index(\"day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_vol =  data.iloc[ : , 0:13].drop(['% change_nifty', '% change_sensex'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3168 entries, 2007-04-19 to 2020-03-16\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Anger                   3168 non-null   float32\n",
      " 1   Anticipation            3168 non-null   float32\n",
      " 2   Disgust                 3168 non-null   float32\n",
      " 3   Fear                    3168 non-null   float32\n",
      " 4   Joy                     3168 non-null   float32\n",
      " 5   Sadness                 3168 non-null   float32\n",
      " 6   Surprise                3168 non-null   float32\n",
      " 7   Trust                   3168 non-null   float32\n",
      " 8   Close_nifty             3168 non-null   float32\n",
      " 9   Close_sensex            3168 non-null   float32\n",
      " 10  Conditional Volatility  3168 non-null   float32\n",
      "dtypes: float32(11)\n",
      "memory usage: 160.9 KB\n"
     ]
    }
   ],
   "source": [
    "data_vol.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Close_nifty</th>\n",
       "      <th>Close_sensex</th>\n",
       "      <th>Conditional Volatility</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>day</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-04-19</th>\n",
       "      <td>0.5000000000</td>\n",
       "      <td>2.2100000381</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.5160000324</td>\n",
       "      <td>0.8130000234</td>\n",
       "      <td>0.4309999943</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0470000505</td>\n",
       "      <td>3,997.6499023438</td>\n",
       "      <td>13,619.7001953125</td>\n",
       "      <td>1.9240180254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-23</th>\n",
       "      <td>0.4850000143</td>\n",
       "      <td>1.3819999695</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.5780000091</td>\n",
       "      <td>1.0479999781</td>\n",
       "      <td>2.6700000763</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.1720000505</td>\n",
       "      <td>4,085.1000976562</td>\n",
       "      <td>13,928.3300781250</td>\n",
       "      <td>1.9290779829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-04-30</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>2.7019999027</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.2969999909</td>\n",
       "      <td>1.6189999580</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.6410000324</td>\n",
       "      <td>4,087.8999023438</td>\n",
       "      <td>13,872.3701171875</td>\n",
       "      <td>2.1650118828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-07</th>\n",
       "      <td>0.5310000181</td>\n",
       "      <td>3.1619999409</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.2969999909</td>\n",
       "      <td>2.6429998875</td>\n",
       "      <td>0.2029999942</td>\n",
       "      <td>0.3670000136</td>\n",
       "      <td>3.5710000992</td>\n",
       "      <td>4,111.1499023438</td>\n",
       "      <td>13,879.2500000000</td>\n",
       "      <td>2.0097639561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05-08</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0069999695</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.8930000067</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>1.0470000505</td>\n",
       "      <td>4,077.0000000000</td>\n",
       "      <td>13,765.4599609375</td>\n",
       "      <td>1.8497519493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Anger  Anticipation      Disgust         Fear          Joy  \\\n",
       "day                                                                            \n",
       "2007-04-19 0.5000000000  2.2100000381 0.0000000000 1.5160000324 0.8130000234   \n",
       "2007-04-23 0.4850000143  1.3819999695 0.0000000000 0.5780000091 1.0479999781   \n",
       "2007-04-30 0.0000000000  2.7019999027 0.0000000000 0.2969999909 1.6189999580   \n",
       "2007-05-07 0.5310000181  3.1619999409 0.0000000000 0.2969999909 2.6429998875   \n",
       "2007-05-08 0.0000000000  1.0069999695 0.0000000000 0.0000000000 0.8930000067   \n",
       "\n",
       "                Sadness     Surprise        Trust      Close_nifty  \\\n",
       "day                                                                  \n",
       "2007-04-19 0.4309999943 0.0000000000 1.0470000505 3,997.6499023438   \n",
       "2007-04-23 2.6700000763 0.0000000000 1.1720000505 4,085.1000976562   \n",
       "2007-04-30 0.0000000000 0.0000000000 1.6410000324 4,087.8999023438   \n",
       "2007-05-07 0.2029999942 0.3670000136 3.5710000992 4,111.1499023438   \n",
       "2007-05-08 0.0000000000 0.0000000000 1.0470000505 4,077.0000000000   \n",
       "\n",
       "                Close_sensex  Conditional Volatility  \n",
       "day                                                   \n",
       "2007-04-19 13,619.7001953125            1.9240180254  \n",
       "2007-04-23 13,928.3300781250            1.9290779829  \n",
       "2007-04-30 13,872.3701171875            2.1650118828  \n",
       "2007-05-07 13,879.2500000000            2.0097639561  \n",
       "2007-05-08 13,765.4599609375            1.8497519493  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_dataset(data):\n",
    "# \t# split into  months\n",
    "#     train, test = data[0:3138], data[3138:3168]\n",
    "# \t# restructure into windows of monthly data\n",
    "#     train = array(split(train, len(train)/30))\n",
    "#     test = array(split(test, len(test)/30))\n",
    "#     return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yscaler = MinMaxScaler()\n",
    "df = pd.DataFrame(data_vol.iloc[18:3138, 10])\n",
    "df = pd.DataFrame(yscaler.fit_transform(df), columns=df.columns, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train, test = data_vol[18:3138], data_vol[3138:3168]\n",
    "train = pd.DataFrame(scaler.fit_transform(train), columns=train.columns, index=train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(scaler.fit_transform(test), columns=test.columns, index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Close_nifty</th>\n",
       "      <th>Close_sensex</th>\n",
       "      <th>Conditional Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "      <td>3,120.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0667189583</td>\n",
       "      <td>0.1310477257</td>\n",
       "      <td>0.0656884834</td>\n",
       "      <td>0.0793018043</td>\n",
       "      <td>0.1180894151</td>\n",
       "      <td>0.1094094813</td>\n",
       "      <td>0.0957049057</td>\n",
       "      <td>0.0974604413</td>\n",
       "      <td>0.4660172760</td>\n",
       "      <td>0.4551837146</td>\n",
       "      <td>0.0627599508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.1343169659</td>\n",
       "      <td>0.1967794001</td>\n",
       "      <td>0.1465145200</td>\n",
       "      <td>0.1510540098</td>\n",
       "      <td>0.1811063737</td>\n",
       "      <td>0.1672122926</td>\n",
       "      <td>0.1806580275</td>\n",
       "      <td>0.1728436798</td>\n",
       "      <td>0.2494771630</td>\n",
       "      <td>0.2389431000</td>\n",
       "      <td>0.1095687598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0102568883</td>\n",
       "      <td>0.0386259221</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0146211691</td>\n",
       "      <td>0.0332342461</td>\n",
       "      <td>0.0263262000</td>\n",
       "      <td>0.0130911404</td>\n",
       "      <td>0.0220476161</td>\n",
       "      <td>0.2735500485</td>\n",
       "      <td>0.2722832114</td>\n",
       "      <td>0.0140771582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0205137767</td>\n",
       "      <td>0.0591193344</td>\n",
       "      <td>0.0147001697</td>\n",
       "      <td>0.0273765000</td>\n",
       "      <td>0.0526462533</td>\n",
       "      <td>0.0504153557</td>\n",
       "      <td>0.0318445377</td>\n",
       "      <td>0.0353673287</td>\n",
       "      <td>0.3654592931</td>\n",
       "      <td>0.3632382154</td>\n",
       "      <td>0.0270586554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0390222305</td>\n",
       "      <td>0.1055044997</td>\n",
       "      <td>0.0361902015</td>\n",
       "      <td>0.0472020544</td>\n",
       "      <td>0.0934748799</td>\n",
       "      <td>0.0913505517</td>\n",
       "      <td>0.0642666277</td>\n",
       "      <td>0.0630328879</td>\n",
       "      <td>0.6322714686</td>\n",
       "      <td>0.6030966938</td>\n",
       "      <td>0.0569391027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.9999999404</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Anger     Anticipation          Disgust             Fear  \\\n",
       "count 3,120.0000000000 3,120.0000000000 3,120.0000000000 3,120.0000000000   \n",
       "mean      0.0667189583     0.1310477257     0.0656884834     0.0793018043   \n",
       "std       0.1343169659     0.1967794001     0.1465145200     0.1510540098   \n",
       "min       0.0000000000     0.0000000000     0.0000000000     0.0000000000   \n",
       "25%       0.0102568883     0.0386259221     0.0000000000     0.0146211691   \n",
       "50%       0.0205137767     0.0591193344     0.0147001697     0.0273765000   \n",
       "75%       0.0390222305     0.1055044997     0.0361902015     0.0472020544   \n",
       "max       0.9999999404     1.0000000000     1.0000000000     1.0000000000   \n",
       "\n",
       "                   Joy          Sadness         Surprise            Trust  \\\n",
       "count 3,120.0000000000 3,120.0000000000 3,120.0000000000 3,120.0000000000   \n",
       "mean      0.1180894151     0.1094094813     0.0957049057     0.0974604413   \n",
       "std       0.1811063737     0.1672122926     0.1806580275     0.1728436798   \n",
       "min       0.0000000000     0.0000000000     0.0000000000     0.0000000000   \n",
       "25%       0.0332342461     0.0263262000     0.0130911404     0.0220476161   \n",
       "50%       0.0526462533     0.0504153557     0.0318445377     0.0353673287   \n",
       "75%       0.0934748799     0.0913505517     0.0642666277     0.0630328879   \n",
       "max       1.0000000000     1.0000000000     1.0000000000     1.0000000000   \n",
       "\n",
       "           Close_nifty     Close_sensex  Conditional Volatility  \n",
       "count 3,120.0000000000 3,120.0000000000        3,120.0000000000  \n",
       "mean      0.4660172760     0.4551837146            0.0627599508  \n",
       "std       0.2494771630     0.2389431000            0.1095687598  \n",
       "min       0.0000000000     0.0000000000            0.0000000000  \n",
       "25%       0.2735500485     0.2722832114            0.0140771582  \n",
       "50%       0.3654592931     0.3632382154            0.0270586554  \n",
       "75%       0.6322714686     0.6030966938            0.0569391027  \n",
       "max       1.0000000000     1.0000000000            1.0000000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Anger</th>\n",
       "      <th>Anticipation</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "      <th>Trust</th>\n",
       "      <th>Close_nifty</th>\n",
       "      <th>Close_sensex</th>\n",
       "      <th>Conditional Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "      <td>30.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.5515988469</td>\n",
       "      <td>0.5071460009</td>\n",
       "      <td>0.4846850038</td>\n",
       "      <td>0.5973227024</td>\n",
       "      <td>0.5359319448</td>\n",
       "      <td>0.4792217612</td>\n",
       "      <td>0.4619318247</td>\n",
       "      <td>0.3523563147</td>\n",
       "      <td>0.7586560845</td>\n",
       "      <td>0.7619783282</td>\n",
       "      <td>0.1437718421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.2692614794</td>\n",
       "      <td>0.2503173649</td>\n",
       "      <td>0.2438482940</td>\n",
       "      <td>0.2421970218</td>\n",
       "      <td>0.2378204912</td>\n",
       "      <td>0.2652696371</td>\n",
       "      <td>0.2123727500</td>\n",
       "      <td>0.2050618678</td>\n",
       "      <td>0.2697877586</td>\n",
       "      <td>0.2673042417</td>\n",
       "      <td>0.2423311025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "      <td>0.0000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.3426034003</td>\n",
       "      <td>0.3419924080</td>\n",
       "      <td>0.2841547430</td>\n",
       "      <td>0.4213398844</td>\n",
       "      <td>0.3571308702</td>\n",
       "      <td>0.2642595172</td>\n",
       "      <td>0.3399322331</td>\n",
       "      <td>0.2331271619</td>\n",
       "      <td>0.6713703275</td>\n",
       "      <td>0.6815431118</td>\n",
       "      <td>0.0386144165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.5611848235</td>\n",
       "      <td>0.5451586843</td>\n",
       "      <td>0.5261563659</td>\n",
       "      <td>0.6132943630</td>\n",
       "      <td>0.5389156938</td>\n",
       "      <td>0.4636054635</td>\n",
       "      <td>0.4811083972</td>\n",
       "      <td>0.3379986882</td>\n",
       "      <td>0.8507556915</td>\n",
       "      <td>0.8544769287</td>\n",
       "      <td>0.0506458730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.7835264802</td>\n",
       "      <td>0.6754058301</td>\n",
       "      <td>0.6659611762</td>\n",
       "      <td>0.7706734836</td>\n",
       "      <td>0.7058766186</td>\n",
       "      <td>0.6165616512</td>\n",
       "      <td>0.5351662338</td>\n",
       "      <td>0.4581854045</td>\n",
       "      <td>0.9620062113</td>\n",
       "      <td>0.9583874941</td>\n",
       "      <td>0.1325604618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>0.9999999404</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "      <td>1.0000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Anger  Anticipation       Disgust          Fear           Joy  \\\n",
       "count 30.0000000000 30.0000000000 30.0000000000 30.0000000000 30.0000000000   \n",
       "mean   0.5515988469  0.5071460009  0.4846850038  0.5973227024  0.5359319448   \n",
       "std    0.2692614794  0.2503173649  0.2438482940  0.2421970218  0.2378204912   \n",
       "min    0.0000000000  0.0000000000  0.0000000000  0.0000000000  0.0000000000   \n",
       "25%    0.3426034003  0.3419924080  0.2841547430  0.4213398844  0.3571308702   \n",
       "50%    0.5611848235  0.5451586843  0.5261563659  0.6132943630  0.5389156938   \n",
       "75%    0.7835264802  0.6754058301  0.6659611762  0.7706734836  0.7058766186   \n",
       "max    1.0000000000  1.0000000000  1.0000000000  1.0000000000  1.0000000000   \n",
       "\n",
       "            Sadness      Surprise         Trust   Close_nifty  Close_sensex  \\\n",
       "count 30.0000000000 30.0000000000 30.0000000000 30.0000000000 30.0000000000   \n",
       "mean   0.4792217612  0.4619318247  0.3523563147  0.7586560845  0.7619783282   \n",
       "std    0.2652696371  0.2123727500  0.2050618678  0.2697877586  0.2673042417   \n",
       "min    0.0000000000  0.0000000000  0.0000000000  0.0000000000  0.0000000000   \n",
       "25%    0.2642595172  0.3399322331  0.2331271619  0.6713703275  0.6815431118   \n",
       "50%    0.4636054635  0.4811083972  0.3379986882  0.8507556915  0.8544769287   \n",
       "75%    0.6165616512  0.5351662338  0.4581854045  0.9620062113  0.9583874941   \n",
       "max    0.9999999404  1.0000000000  1.0000000000  1.0000000000  1.0000000000   \n",
       "\n",
       "       Conditional Volatility  \n",
       "count           30.0000000000  \n",
       "mean             0.1437718421  \n",
       "std              0.2423311025  \n",
       "min              0.0000000000  \n",
       "25%              0.0386144165  \n",
       "50%              0.0506458730  \n",
       "75%              0.1325604618  \n",
       "max              1.0000000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = array(split(train.values, len(train)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = array(split(test.values, len(test)/30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# for col in data_vol.columns :\n",
    "# # for i in range (0,train.shape[2]) :\n",
    "#     data_vol[[col]] = scaler.fit_transform(data_vol[[col]])\n",
    "# #     train[:,:, i] = scaler.fit_transform(train[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_supervised(train, n_input, n_out=30):\n",
    "\t# flatten data\n",
    "\tdata = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n",
    "\tX, y = list(), list()\n",
    "\tin_start = 0\n",
    "\tfor _ in range(len(data)):\n",
    "\t\tin_end = in_start + n_input\n",
    "\t\tout_end = in_end + n_out\n",
    "\t\tif out_end <= len(data):\n",
    "\t\t\tX.append(data[in_start:in_end, :])\n",
    "\t\t\ty.append(data[in_end:out_end, 10])\n",
    "\t\tin_start += 1\n",
    "\treturn array(X), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train, n_input):\n",
    "\ttrain_x, train_y = to_supervised(train, n_input)\n",
    "\tverbose, epochs, batch_size = 1, 300, 100\n",
    "\tn_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n",
    "\t# reshape output into [samples, timesteps, features]\n",
    "\ttrain_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n",
    "\tprint(train_x.shape)\n",
    "\tprint(train_y.shape)\n",
    "    \n",
    "\t# define model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(LSTM(400, activation='relu', input_shape=(n_timesteps, n_features)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(RepeatVector(n_outputs))\n",
    "\tmodel.add(LSTM(400, activation='relu', return_sequences=True))\n",
    "\tmodel.add(TimeDistributed(Dense(100, activation='relu')))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(TimeDistributed(Dense(1)))\n",
    "\tmodel.compile(loss='mse', optimizer='adam')\n",
    "\tmodel.summary()\n",
    "    \n",
    "\t# fit network\n",
    "\tmodel.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose, shuffle= False )\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3061, 30, 11)\n",
      "(3061, 30, 1)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 400)               659200    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 30, 400)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 400)           1281600   \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 30, 100)           40100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 30, 100)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 30, 1)             101       \n",
      "=================================================================\n",
      "Total params: 1,981,001\n",
      "Trainable params: 1,981,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/300\n",
      "3061/3061 [==============================] - 37s 12ms/step - loss: 0.0179\n",
      "Epoch 2/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0149\n",
      "Epoch 3/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0134\n",
      "Epoch 4/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0100\n",
      "Epoch 5/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0124\n",
      "Epoch 6/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0117\n",
      "Epoch 7/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0102\n",
      "Epoch 8/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0082\n",
      "Epoch 9/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0330\n",
      "Epoch 10/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0115\n",
      "Epoch 11/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0105\n",
      "Epoch 12/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0092\n",
      "Epoch 13/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0084\n",
      "Epoch 14/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0082\n",
      "Epoch 15/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0079\n",
      "Epoch 16/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0069\n",
      "Epoch 17/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0061\n",
      "Epoch 18/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0070\n",
      "Epoch 19/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0060\n",
      "Epoch 20/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0064\n",
      "Epoch 21/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0058\n",
      "Epoch 22/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0060\n",
      "Epoch 23/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0057\n",
      "Epoch 24/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0062\n",
      "Epoch 25/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0057\n",
      "Epoch 26/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0062\n",
      "Epoch 27/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0056\n",
      "Epoch 28/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0057\n",
      "Epoch 29/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0054\n",
      "Epoch 30/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0055\n",
      "Epoch 31/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0059\n",
      "Epoch 32/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0055\n",
      "Epoch 33/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0054\n",
      "Epoch 34/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 35/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0058\n",
      "Epoch 36/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 37/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0058\n",
      "Epoch 38/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0053\n",
      "Epoch 39/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0057\n",
      "Epoch 40/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0052\n",
      "Epoch 41/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0053\n",
      "Epoch 42/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0052\n",
      "Epoch 43/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0061\n",
      "Epoch 44/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0052\n",
      "Epoch 45/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0054\n",
      "Epoch 46/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0051\n",
      "Epoch 47/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0056\n",
      "Epoch 48/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0051\n",
      "Epoch 49/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0056\n",
      "Epoch 50/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0050\n",
      "Epoch 51/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0051\n",
      "Epoch 52/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0052\n",
      "Epoch 53/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0066\n",
      "Epoch 54/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0054\n",
      "Epoch 55/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0051\n",
      "Epoch 56/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0049\n",
      "Epoch 57/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0048\n",
      "Epoch 58/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0047\n",
      "Epoch 59/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0061\n",
      "Epoch 60/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0060\n",
      "Epoch 61/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0065\n",
      "Epoch 62/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0052\n",
      "Epoch 63/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0050\n",
      "Epoch 64/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0049\n",
      "Epoch 65/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0048\n",
      "Epoch 66/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0051\n",
      "Epoch 67/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0084\n",
      "Epoch 68/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0095\n",
      "Epoch 69/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 380718635.3983\n",
      "Epoch 70/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 142182137.2361\n",
      "Epoch 71/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 73670.6850\n",
      "Epoch 72/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0133\n",
      "Epoch 73/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0130\n",
      "Epoch 74/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0122\n",
      "Epoch 75/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0116\n",
      "Epoch 76/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0111\n",
      "Epoch 77/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0104\n",
      "Epoch 78/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0101\n",
      "Epoch 79/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0093\n",
      "Epoch 80/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0089\n",
      "Epoch 81/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0076\n",
      "Epoch 82/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0073\n",
      "Epoch 83/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0067\n",
      "Epoch 84/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0063\n",
      "Epoch 85/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0062\n",
      "Epoch 86/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0062\n",
      "Epoch 87/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0060\n",
      "Epoch 88/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0066\n",
      "Epoch 89/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0059\n",
      "Epoch 90/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0063\n",
      "Epoch 91/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0061\n",
      "Epoch 92/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0062\n",
      "Epoch 93/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0057\n",
      "Epoch 94/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0059\n",
      "Epoch 95/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0058\n",
      "Epoch 96/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0057\n",
      "Epoch 97/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0058\n",
      "Epoch 98/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0059\n",
      "Epoch 99/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0058\n",
      "Epoch 100/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0060\n",
      "Epoch 101/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0055\n",
      "Epoch 102/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0055\n",
      "Epoch 103/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 104/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0055\n",
      "Epoch 105/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 106/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0055\n",
      "Epoch 107/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0054\n",
      "Epoch 108/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0062\n",
      "Epoch 109/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0055\n",
      "Epoch 110/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0054\n",
      "Epoch 111/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0052\n",
      "Epoch 112/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0053\n",
      "Epoch 113/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0053\n",
      "Epoch 114/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0054\n",
      "Epoch 115/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0053\n",
      "Epoch 116/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 117/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0051\n",
      "Epoch 118/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0055\n",
      "Epoch 119/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0052\n",
      "Epoch 120/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0055\n",
      "Epoch 121/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0052\n",
      "Epoch 122/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0055\n",
      "Epoch 123/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0052\n",
      "Epoch 124/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0056\n",
      "Epoch 125/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0051\n",
      "Epoch 126/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0051\n",
      "Epoch 127/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0050\n",
      "Epoch 128/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0052\n",
      "Epoch 129/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0052\n",
      "Epoch 130/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0054\n",
      "Epoch 131/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0050\n",
      "Epoch 132/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0053\n",
      "Epoch 133/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0051\n",
      "Epoch 134/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0051\n",
      "Epoch 135/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0049\n",
      "Epoch 136/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0050\n",
      "Epoch 137/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0050\n",
      "Epoch 138/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0051\n",
      "Epoch 139/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0048\n",
      "Epoch 140/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0050\n",
      "Epoch 141/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0050\n",
      "Epoch 142/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0050\n",
      "Epoch 143/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0050\n",
      "Epoch 144/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0053\n",
      "Epoch 145/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0048\n",
      "Epoch 146/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0051\n",
      "Epoch 147/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0048\n",
      "Epoch 148/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0050\n",
      "Epoch 149/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0050\n",
      "Epoch 150/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0053\n",
      "Epoch 151/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0049\n",
      "Epoch 152/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0048\n",
      "Epoch 153/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0046\n",
      "Epoch 154/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0047\n",
      "Epoch 155/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0047\n",
      "Epoch 156/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0050\n",
      "Epoch 157/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0048\n",
      "Epoch 158/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0052\n",
      "Epoch 159/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0046\n",
      "Epoch 160/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0047\n",
      "Epoch 161/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0046\n",
      "Epoch 162/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0048\n",
      "Epoch 163/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0046\n",
      "Epoch 164/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0047\n",
      "Epoch 165/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0047\n",
      "Epoch 166/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0047\n",
      "Epoch 167/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0045\n",
      "Epoch 168/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0046\n",
      "Epoch 169/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0044\n",
      "Epoch 170/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0047\n",
      "Epoch 171/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0046\n",
      "Epoch 172/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0051\n",
      "Epoch 173/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0046\n",
      "Epoch 174/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0045\n",
      "Epoch 175/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0043\n",
      "Epoch 176/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0042\n",
      "Epoch 177/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0042\n",
      "Epoch 178/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0047\n",
      "Epoch 179/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0044\n",
      "Epoch 180/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0048\n",
      "Epoch 181/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0042\n",
      "Epoch 182/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0042\n",
      "Epoch 183/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0043\n",
      "Epoch 184/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0045\n",
      "Epoch 185/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0043\n",
      "Epoch 186/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0047\n",
      "Epoch 187/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0044\n",
      "Epoch 188/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0039\n",
      "Epoch 194/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0042\n",
      "Epoch 195/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0042\n",
      "Epoch 196/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 197/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0041\n",
      "Epoch 198/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0039\n",
      "Epoch 199/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0041\n",
      "Epoch 200/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0038\n",
      "Epoch 201/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0040\n",
      "Epoch 202/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0043\n",
      "Epoch 203/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0046\n",
      "Epoch 204/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0057\n",
      "Epoch 205/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0048\n",
      "Epoch 206/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0045\n",
      "Epoch 207/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0043\n",
      "Epoch 208/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0038\n",
      "Epoch 209/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0039\n",
      "Epoch 210/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0039\n",
      "Epoch 211/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0037\n",
      "Epoch 212/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0038\n",
      "Epoch 213/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0038\n",
      "Epoch 214/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0044\n",
      "Epoch 215/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0041\n",
      "Epoch 216/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0039\n",
      "Epoch 217/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0039\n",
      "Epoch 218/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0046\n",
      "Epoch 219/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0036\n",
      "Epoch 220/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0037\n",
      "Epoch 221/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0037\n",
      "Epoch 222/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0039\n",
      "Epoch 223/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0036\n",
      "Epoch 224/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0040\n",
      "Epoch 225/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0040\n",
      "Epoch 226/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0044\n",
      "Epoch 227/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0042\n",
      "Epoch 228/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0033\n",
      "Epoch 229/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0034\n",
      "Epoch 230/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0033\n",
      "Epoch 231/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0035\n",
      "Epoch 232/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0035\n",
      "Epoch 233/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0049\n",
      "Epoch 234/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0037\n",
      "Epoch 235/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0035\n",
      "Epoch 236/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0035\n",
      "Epoch 237/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0039\n",
      "Epoch 238/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0044\n",
      "Epoch 239/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0042\n",
      "Epoch 240/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0034\n",
      "Epoch 241/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0036\n",
      "Epoch 242/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0035\n",
      "Epoch 243/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0039\n",
      "Epoch 244/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0037\n",
      "Epoch 245/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0039\n",
      "Epoch 246/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0036\n",
      "Epoch 247/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0041\n",
      "Epoch 248/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0035\n",
      "Epoch 249/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0033\n",
      "Epoch 250/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0033\n",
      "Epoch 251/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0035\n",
      "Epoch 252/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0038\n",
      "Epoch 253/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0035\n",
      "Epoch 254/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0035\n",
      "Epoch 255/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0037\n",
      "Epoch 256/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0038\n",
      "Epoch 257/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0050\n",
      "Epoch 258/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0038\n",
      "Epoch 259/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0033\n",
      "Epoch 260/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0035\n",
      "Epoch 261/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0031\n",
      "Epoch 262/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0033\n",
      "Epoch 263/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0032\n",
      "Epoch 264/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0039\n",
      "Epoch 265/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0036\n",
      "Epoch 266/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0038\n",
      "Epoch 267/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0032\n",
      "Epoch 268/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0031\n",
      "Epoch 269/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0032\n",
      "Epoch 270/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0035\n",
      "Epoch 271/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0033\n",
      "Epoch 272/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0038\n",
      "Epoch 273/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0031\n",
      "Epoch 274/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0030\n",
      "Epoch 275/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0030\n",
      "Epoch 276/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0028\n",
      "Epoch 277/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 1.1437\n",
      "Epoch 278/300\n",
      "3061/3061 [==============================] - 30s 10ms/step - loss: 0.0117\n",
      "Epoch 279/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.2550\n",
      "Epoch 280/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0124\n",
      "Epoch 281/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0073\n",
      "Epoch 282/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0086\n",
      "Epoch 283/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0064\n",
      "Epoch 284/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0054\n",
      "Epoch 285/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0046\n",
      "Epoch 286/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0055\n",
      "Epoch 287/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0042\n",
      "Epoch 288/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0040\n",
      "Epoch 289/300\n",
      "3061/3061 [==============================] - 34s 11ms/step - loss: 0.0039\n",
      "Epoch 290/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0041\n",
      "Epoch 291/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0038\n",
      "Epoch 292/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0051\n",
      "Epoch 293/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0039\n",
      "Epoch 294/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0039\n",
      "Epoch 295/300\n",
      "3061/3061 [==============================] - 32s 10ms/step - loss: 0.0037\n",
      "Epoch 296/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0034\n",
      "Epoch 297/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0039\n",
      "Epoch 298/300\n",
      "3061/3061 [==============================] - 33s 11ms/step - loss: 0.0037\n",
      "Epoch 299/300\n",
      "3061/3061 [==============================] - 32s 11ms/step - loss: 0.0042\n",
      "Epoch 300/300\n",
      "3061/3061 [==============================] - 31s 10ms/step - loss: 0.0034\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "n_input = 30 \n",
    "model = build_model(train, n_input) \n",
    "model.save( 'model5-copy3' + '.h5')\n",
    "print(\"model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from keras.models import load_model  \n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model5-copy2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(model, history, n_input):\n",
    "\t# flatten data\n",
    "\tdata = array(history)\n",
    "\tdata = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n",
    "\t# retrieve last observations for input data\n",
    "\tinput_x = data[-n_input:, :]\n",
    "\t# reshape into [1, n_input, n]\n",
    "\tinput_x = input_x.reshape((1, input_x.shape[0], input_x.shape[1]))\n",
    "\t# forecast the next week\n",
    "\tyhat = model.predict(input_x, verbose=1)\n",
    "\t# we only want the vector forecast\n",
    "\tyhat = yhat[0]\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_forecasts_new(actual, predicted):\n",
    "\tscores = list()\n",
    "\t# calculate an RMSE score for each day\n",
    "\tfor i in range(actual.shape[1]):\n",
    "\t\tmse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "\t\trmse = sqrt(mse)\n",
    "\t\t# store\n",
    "\t\tscores.append(rmse)\n",
    "        \n",
    "\t# calculate overall RMSE\n",
    "\ts = 0\n",
    "\tfor row in range(actual.shape[0]):\n",
    "\t\tfor col in range(actual.shape[1]):\n",
    "\t\t\ts += (actual[row, col] - predicted[row, col])**2\n",
    "\tscore = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "\treturn score, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_new(train, test, n_input):\n",
    "    \n",
    "\t# history is a list of weekly data\n",
    "\thistory = [x for x in train]\n",
    "    \n",
    "\t# walk-forward validation \n",
    "\tpredictions = list()\n",
    "    \n",
    "\tfor i in range(len(test)):\n",
    "\t\tyhat_sequence = forecast(model, history, n_input)\n",
    "# \t\tprint(type(yhat_sequence))\n",
    "# \t\tprint(yhat_sequence.shape) \n",
    "# \t\ty1 = scaler.inverse_transform(yhat_sequence)\n",
    "# \t\tprint(type(y1))\n",
    "# \t\tprint(y1.shape)    \n",
    "\t\tpredictions.append(yhat_sequence)\n",
    "\t\thistory.append(test[i, :])\n",
    "        \n",
    "\t# evaluate predictions days for each week\n",
    "\tpredictions = array(predictions)\n",
    "# \tprint(predictions.shape)  \n",
    "# \tpredictions = scaler.inverse_transform(predictions[0,:,:])\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 717ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = evaluate_model_new(train, test, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48979443 0.5681981  0.4947866  0.4829613  0.44494575 0.4523285\n",
      "  0.44238916 0.27872235 0.97078705 0.96971655 0.03399805]]\n"
     ]
    }
   ],
   "source": [
    "print(test[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.01868879]\n",
      "  [0.01977039]\n",
      "  [0.01935527]\n",
      "  [0.01970544]\n",
      "  [0.02059191]\n",
      "  [0.02178053]\n",
      "  [0.02307583]\n",
      "  [0.02329719]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]\n",
      "  [0.02297284]]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, scores = evaluate_forecasts_new(test[:, :, 8], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_scores(name, score, scores):\n",
    "\ts_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "\tprint('%s: [%.3f] %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: [0.783] 0.8, 0.8, 0.9, 0.9, 1.0, 0.9, 0.9, 0.9, 1.0, 1.0, 0.9, 0.9, 0.9, 1.0, 0.9, 0.9, 0.8, 0.8, 0.8, 0.6, 0.6, 0.7, 0.7, 0.7, 0.6, 0.4, 0.4, 0.1, 0.2, 0.0\n"
     ]
    }
   ],
   "source": [
    "summarize_scores('test', score, scores)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
